# 게임 AI 탐색 알고리즘 개발 여정: 개인적 성찰과 통찰

이 회고록은 게임 AI 탐색 알고리즘 프로젝트를 개발하면서 겪은 경험, 배움, 도전, 그리고 통찰을 기록한 것입니다. 기술적 명세와 벤치마크 결과를 넘어, 개발 과정에서의 사고방식과 배움에 초점을 맞추었습니다.

## 목차

1. [개발 동기와 접근 방식](#개발-동기와-접근-방식)
2. [게임 환경별 개발 경험](#게임-환경별-개발-경험)
3. [알고리즘 구현 과정의 통찰](#알고리즘-구현-과정의-통찰)
4. [최적화 여정](#최적화-여정)
5. [코드 구조화와 리팩토링](#코드-구조화와-리팩토링)
6. [프로젝트를 통해 배운 교훈](#프로젝트를-통해-배운-교훈)

## 개발 동기와 접근 방식

이 프로젝트는 다양한 게임 AI 탐색 알고리즘을 실제로 구현하고 비교해보고자 하는 호기심에서 시작되었습니다. 교과서에서 이론으로만 접했던 알고리즘들이 실제 게임 환경에서 어떻게 작동하는지, 그리고 최적화를 통해 얼마나 성능을 개선할 수 있는지 직접 경험하고 싶었습니다.

처음에는 단순한 미로 게임에서 시작하여 점차 다양한 게임 환경과 알고리즘으로 확장해 나갔습니다. 각 환경마다 그에 맞는 최적의 알고리즘을 찾고, 왜 그 알고리즘이 해당 환경에 적합한지 이해하는 과정이 특히 흥미로웠습니다.

<br>

## 게임 환경별 개발 경험

### 1. Maze 게임 (단일 에이전트, 컨텍스트 있음)

Maze 게임은 가장 기본적인 환경이었지만, 여기서도 많은 것을 배웠습니다. 특히 Greedy, Beam Search, Chokudai 알고리즘을 구현하며 각각의 접근 방식 차이를 이해할 수 있었습니다.

초기에는 알고리즘 구현 자체에 집중했지만, 벤치마크를 진행하면서 Chokudai 알고리즘이 단순한 Greedy 접근법보다 훨씬 우수한 성능을 보인다는 점이 인상적이었습니다. 이론적으로는 알고 있었지만, 실제로 확인하니 알고리즘 선택의 중요성을 실감했습니다.

### 2. AutoMaze 게임 (단일 에이전트, 컨텍스트 없음)

AutoMaze에서는 같은 알고리즘을 한 코드가 아닌 분리해서 작성하는 방식이 처음에는 어색했습니다. 헤더(추상화)와 cpp로 직접 로직을 작성하는 방식을 구체적으로 이해하게 되었습니다.

이 환경에서는 3개의 플레이어가 같은 팀을 이루어 작동하며, 랜덤, 언덕 오르기(Hill Climbing), 담금질 기법(Simulated Annealing)과 같은 메타 휴리스틱 알고리즘을 구현했습니다. 특히 담금질 기법 구현 과정에서 지역 최적해(local optima) 문제를 직접 경험하고, 이를 해결하기 위한 방법을 고민했던 것이 매우 유익했습니다.

### 3. TwoMaze 게임 (두 플레이어 교대 행동)

TwoMaze는 게임 이론 관점에서 가장 흥미로운 환경이었습니다. 다음과 같은 중요한 통찰을 얻었습니다:

- **Minimax vs Alpha-Beta**: Minimax보다 Alpha-Beta Pruning이 시간 효율성 측면에서 완전한 상위 호환이었습니다. 이론적으로 알고 있던 내용을 실제로 확인하니, 가지치기의 중요성을 실감했습니다.

- **재귀와 부호 반전의 원리**: Alpha-Beta 가지치기 알고리즘에서 재귀적으로 부호를 반전시키는 원리를 이해하는 데 시간이 걸렸습니다. max(a, b) = -min(-a, -b)라는 수학적 특성을 활용하여, 재귀에서 최대화 문제가 최소화 문제로, 최소화 문제가 최대화 문제로 변환된다는 점을 깨달았습니다.

- **-beta와 -alpha의 의미**: 알파(최솟값 보장)는 다음 레벨에서 -베타(최대 허용값)이 되고, 베타(최대값 보장)는 다음 레벨에서 -alpha(최소 허용값)이 된다는 개념이 처음에는 직관적이지 않았지만, 코드 구현과 실험을 통해 명확해졌습니다.

- **반복 심화 탐색(Iterative Deepening)의 가치**: 현실 세계의 게임 AI는 고정된 깊이로 탐색하기보다는 시간 제한 내에서 최대한 깊이 탐색해야 함을 배웠습니다. 먼저 1 깊이로 탐색한 후 시간이 남으면 2 깊이, 3 깊이로 점진적으로 들어가는 반복 심화 탐색이 실용적으로 더 효율적이었습니다.

- **Thunder 알고리즘의 발견**: MCTS보다 계산 비용이 적으면서도 좋은 성능을 보이는 Thunder 알고리즘을 분석하는 과정이 흥미로웠습니다. 플레이아웃 단계에서 시간이 많이 소요되는 MCTS의 한계를 극복하기 위한 접근법이었습니다.

- **상태 관리의 중요성**: 출력이 이상하거나 승률 계산이 잘못되는 등의 문제를 겪으면서, 턴 교환을 고려한 상태 관리의 중요성을 배웠습니다. 특히 `getWinningStatus`와 `getScore` 함수가 턴 교환을 제대로 반영하지 못했던 부분을 수정하면서 많은 것을 배웠습니다.

### 4. SimMaze 게임 (두 플레이어 동시 행동)

SimMaze는 개발 과정에서 가장 도전적인 환경이었습니다. 두 플레이어가 동시에 행동을 결정하는 환경에서, 기존의 교대 행동 알고리즘이 그대로 적용되지 않는다는 점이 흥미로웠습니다.

이 환경에서 얻은 중요한 통찰:
- 동시에 행동하는 경우, 점수를 함께 먹을 수 있기 때문에, 한쪽이 이득이고 반대쪽이 손해를 보는 제로섬 게임과는 다른 방식의 탐색이 필요했습니다.
- 상대방의 행동을 미리 알고 있는지 여부가 탐색 효율에 큰 영향을 미친다는 점을 발견했습니다.
- DUCT 알고리즘은 한 노드가 양쪽 플레이어의 정보를 모두 가지며, 이는 노드 수가 N×M(두 플레이어의 모든 경우의 수)으로 증가함을 의미했습니다. 이로 인해 MCTS나 순수 몬테카를로보다 더 많은 시행 횟수가 필요했습니다.

<br>

## 알고리즘 구현 과정의 통찰

서로 다른 게임 환경에 맞는 알고리즘을 구현하면서 얻은 통찰들을 공유합니다:

### 단일 에이전트 알고리즘

단일 에이전트 알고리즘을 구현하면서, 가장 인상적이었던 것은 단순한 방법과 복잡한 방법 사이의 성능 차이였습니다. Greedy가 간단하고 계산 비용이 낮음에도 불구하고, Beam Search나 Chokudai가 훨씬 더 나은 결과를 가져온다는 점은 "계산 리소스를 더 투자할 가치가 있는가?"라는 질문을 던지게 했습니다.

특히 Beam Search의 폭과 깊이 매개변수 조정이 성능에 큰 영향을 미친다는 점을 발견했습니다. 최적의 매개변수를 찾기 위한 실험을 통해 성능과 계산 비용 사이의 균형을 이해하게 되었습니다.

### 경로 탐색 알고리즘

벽이 있는 미로(WallMaze) 환경에서 다양한 경로 탐색 알고리즘을 구현하면서 다음과 같은 통찰을 얻었습니다:

- BFS는 가장 기본적이지만 매우 효율적인 알고리즘입니다. 특히 모든 간선의 가중치가 동일할 때는 최단 경로를 보장합니다.
- DFS는 메모리 사용이 적지만, 최단 경로를 보장하지 않기 때문에 이 문제에는 적합하지 않았습니다. 실제로 DFS로 찾은 경로가 가장 가까운 점수가 아닐 수 있음을 확인했습니다.
- A*는 휴리스틱 함수의 선택에 따라 성능이 크게 달라집니다. 좋은 휴리스틱은 탐색 공간을 크게 줄일 수 있다는 것을 직접 경험했습니다.
- 단순히 거리만 고려하는 것이 아니라, 점수도 함께 고려하는 Value-Based A* 알고리즘이 더 효율적이었습니다. 이를 통해 휴리스틱 함수 설계의 중요성을 다시 한번 느꼈습니다.

<br>

## 최적화 여정

이 프로젝트에서 가장 도전적이고 보람 있었던 부분은 다양한 최적화 기법을 적용해 성능을 향상시키는 과정이었습니다.

### 비트보드 최적화

비트보드 최적화는 가장 극적인 성능 향상을 가져왔습니다. 특히 WallMaze에서 BFS를 비트 연산으로 대체했을 때의 경험은 매우 인상적이었습니다:

- 비트 시프트와 비트 연산을 통해 한 번에 여러 위치를 처리할 수 있다는 점이 핵심이었습니다.
- 상하좌우 이동을 OR 연산으로 표현하는 방식이 처음에는 직관적이지 않았지만, 구현하고 나서는 그 우아함과 효율성에 놀랐습니다.

비트보드의 공간 복잡도가 O(H*W)에서 O(1)로, 시간 복잡도가 O(H*W)에서 O(D)(D는 거리)로 줄어든 것은 이론적으로 알고 있었지만, 실제로 경험해보니 그 효과가 더욱 체감되었습니다.

한 가지 흥미로운 도전은 한 개의 비트열로 2D 배열을 표현하는 방법이었습니다. 2차원 배열을 1차원 배열로 표현하는 것과 유사했지만, 상하 이동이 복잡했습니다. 위쪽 연산은, 첫 번째 열부터 다음 것과 OR 연산을, 아래 이동은 마지막 열부터 OR 연산을 수행하는 방식으로 해결했습니다. 이 과정에서 비트마스크가 주변 열에 영향을 주지 않도록 하는 것이 중요했습니다.

### Zobrist 해싱

Zobrist 해싱을 통한 상태 중복 제거도 인상적인 최적화 경험이었습니다:

- 칸으로 나뉜 게임판에서 물건이 배치되는 형태의 게임에 이 해싱 알고리즘이 특히 효과적임을 알게 되었습니다.
- XOR 연산의 특성(자기 자신과 XOR하면 0이 됨)을 활용하여, 게임 상태가 변할 때마다 전체 해시를 다시 계산하지 않고 변경된 부분만 업데이트할 수 있었습니다.
- 플레이어 이동 2회와 점수 변경 1회, 총 3회의 XOR 연산만으로 새로운 해시 값을 계산할 수 있었고, 이는 매번 H*W 해시를 계산하는 것보다 훨씬 효율적이었습니다.

<br>

## 코드 구조화와 리팩토링

프로젝트가 커지면서 코드 구조화와 리팩토링의 중요성을 절실히 느꼈습니다:

### 초기 구조화의 어려움

처음에는 여러 게임과 알고리즘을 어떻게 구조화할지 많은 고민이 있었습니다. 같은 게임이라도 규칙이 다르고 필요한 상수도 달라져 코드가 복잡해지는 문제에 직면했습니다. 중복을 제거하고 코드의 재사용성을 높이는 것이 가장 큰 과제였습니다.

### 리팩토링 과정과 교훈

여러 차례의 리팩토링을 통해 다음과 같은 개선을 이루었습니다:

1. **중복 제거**: 템플릿 기반 유틸리티 함수를 추가하여 좌표 유효성 검사, 랜덤 점수 맵 생성, 캐릭터 렌더링 함수 등을 공통화했습니다.
2. **Maze에 걸린 별칭 제거**: 템플릿을 활용해 함수들이 모든 환경에서 일관되게 작동하도록 수정했습니다.
3. **탐색 설정 통합**: 다양한 탐색 알고리즘의 설정을 하나로 통합했습니다.
4. **계층적 Makefile**: 여러 게임을 효율적으로 관리하기 위해 계층적인 Makefile 구조를 도입했습니다.

### 인터페이스 도입

가장 큰 리팩토링은 모든 알고리즘과 게임 상태를 위한 공통 인터페이스를 도입한 것이었습니다:

- 모든 알고리즘이 "Algorithm" 인터페이스를 구현하고, 모든 게임이 "GameState" 인터페이스를 구현하도록 했습니다.
- 팩토리 패턴을 통해 일관된 객체 생성 방식을 제공했습니다.
- 이전에는 데모 코드에서 직접 알고리즘을 호출했지만, 리팩토링 후에는 AlgorithmFactory를 통해 알고리즘 객체를 생성하고 이를 통해 알고리즘을 실행하도록 변경했습니다.

이러한 구조 개선은 시간이 많이 걸렸지만, 코드의 유지보수성과 확장성을 크게 향상시켰습니다. 특히 새로운 게임이나 알고리즘을 추가할 때의 편의성이 크게 증가했습니다.

<br>

## 프로젝트를 통해 배운 교훈

이 프로젝트를 통해 얻은 주요 교훈을 공유합니다:

### 기술적 교훈

1. **알고리즘 선택의 중요성**: 같은 문제라도 알고리즘에 따라 성능이 크게 달라질 수 있습니다. 환경과 요구사항에 맞는 알고리즘을 선택하는 것이 중요합니다.

2. **최적화의 가치**: 비트보드나 해싱과 같은 최적화 기법은 성능을 극적으로 향상시킬 수 있습니다. 특히 성능이 중요한 게임 AI에서는 이러한 최적화가 필수적입니다.

3. **휴리스틱 설계의 영향**: A* 알고리즘에서 보았듯이, 좋은 휴리스틱 함수는 탐색 효율성을 크게 높일 수 있습니다. 도메인 지식을 활용한 휴리스틱 설계가 중요합니다.

4. **시뮬레이션 기반 접근법의 강점**: MCTS, DUCT와 같은 시뮬레이션 기반 알고리즘은 복잡한 게임 환경에서 훌륭한 성능을 보였습니다. 특히 완전한 탐색이 불가능한 상황에서 유용했습니다.

### 개발 프로세스 교훈

1. **테스트의 중요성**: 특히 알고리즘 구현에서 테스트는 매우 중요합니다. 처음부터 자동화된 테스트를 구현했더라면 디버깅 시간을 크게 줄일 수 있었을 것입니다.

2. **리팩토링의 가치**: 초기에는 리팩토링에 시간을 투자하는 것이 비효율적으로 느껴졌지만, 결과적으로는 코드 품질과 개발 속도를 크게 향상시켰습니다.

3. **설계 패턴의 적용**: 팩토리 패턴, 전략 패턴 등의 설계 패턴을 적용함으로써 코드의 유연성과 확장성이 크게 향상되었습니다.

4. **성능 측정의 중요성**: 객관적인 벤치마크를 통해 알고리즘의 성능을 측정함으로써, 주관적인 판단이 아닌 데이터에 기반한 의사결정을 할 수 있었습니다.

### 개인적 성장

이 프로젝트를 통해 게임 AI 알고리즘에 대한 이론적 이해를 넘어, 실제 구현과 최적화 경험을 쌓을 수 있었습니다. 특히 비트 연산, 메모리 최적화, 알고리즘 분석 등의 실질적인 기술을 습득했습니다.

또한, 복잡한 프로젝트를 체계적으로 구조화하고 관리하는 방법을 배웠으며, 이는 향후 더 큰 규모의 프로젝트를 진행할 때 큰 도움이 될 것입니다.

<br>

## 맺음말

이 프로젝트는 단순한 알고리즘 구현을 넘어, 게임 AI의 깊이 있는 이해와 최적화 기법의 실제 적용을 경험하는 여정이었습니다. 이론과 실제의 간극을 메우는 과정에서 많은 도전과 발견이 있었고, 이를 통해 더 나은 개발자로 성장할 수 있었습니다.

앞으로도 이 프로젝트를 확장하여 더 복잡한 게임 환경과 알고리즘을 탐구하고, 강화학습과 같은 새로운 접근법도 통합해 볼 계획입니다. 게임 AI는 끊임없이 발전하는 분야이며, 이 여정은 계속될 것입니다.